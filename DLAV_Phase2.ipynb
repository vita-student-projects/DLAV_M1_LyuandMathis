{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg5EMWEn5zmH"
      },
      "source": [
        "# Phase 2: Trajectory Prediction with Auxiliary Depth Estimation\n",
        "\n",
        "# 🧭 Introduction\n",
        "\n",
        "\"\"\"\n",
        "Welcome to **Phase 2** of the DLAV Projec! 🚗💨\n",
        "\n",
        "In this phase, you'll work with a more challenging dataset that includes:\n",
        "- RGB **camera images**\n",
        "- Ground-truth **depth maps**\n",
        "- Ground-truth **semantic segmentation** labels\n",
        "\n",
        "Your goal is still to predict the **future trajectory** of the self-driving car (SDC), but you now have more tools at your disposal! 🎯\n",
        "\n",
        "Here, we provide an example where **depth estimation** is used as an auxiliary task to improve trajectory prediction.\n",
        "\n",
        "However, you're **free to explore** other auxiliary tasks (e.g., using semantic labels), different loss functions, data augmentations, or better architectures! 💡\n",
        "\n",
        "This notebook will walk you through loading the dataset, building a model, training with and without the auxiliary task, and visualizing results.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ulknsOA5x7-",
        "outputId": "4f3e052b-8a56-44c9-9e5e-330fca8ba5c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1YkGwaxBKNiYL2nq--cB6WMmYGzRmRKVr\n",
            "From (redirected): https://drive.google.com/uc?id=1YkGwaxBKNiYL2nq--cB6WMmYGzRmRKVr&confirm=t&uuid=ce3622bc-785a-4b1b-9470-ca5685617e75\n",
            "To: /content/dlav_train.zip\n",
            "100%|██████████| 439M/439M [00:07<00:00, 57.8MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1wtmT_vH9mMUNOwrNOMFP6WFw6e8rbOdu\n",
            "From (redirected): https://drive.google.com/uc?id=1wtmT_vH9mMUNOwrNOMFP6WFw6e8rbOdu&confirm=t&uuid=0f8f6018-ac89-4392-88f4-a9ba9c608d2c\n",
            "To: /content/dlav_val.zip\n",
            "100%|██████████| 87.8M/87.8M [00:01<00:00, 71.0MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1G9xGE7s-Ikvvc2-LZTUyuzhWAlNdLTLV\n",
            "From (redirected): https://drive.google.com/uc?id=1G9xGE7s-Ikvvc2-LZTUyuzhWAlNdLTLV&confirm=t&uuid=c8f8b8ab-9f04-422d-b00a-95e00792340e\n",
            "To: /content/dlav_test_public.zip\n",
            "100%|██████████| 86.6M/86.6M [00:01<00:00, 86.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Install gdown to handle Google Drive file download\n",
        "!pip install -q gdown\n",
        "\n",
        "import gdown\n",
        "import zipfile\n",
        "\n",
        "download_url = f\"https://drive.google.com/uc?id=1YkGwaxBKNiYL2nq--cB6WMmYGzRmRKVr\"\n",
        "output_zip = \"dlav_train.zip\"\n",
        "gdown.download(download_url, output_zip, quiet=False)  # Downloads the file to your drive\n",
        "with zipfile.ZipFile(output_zip, 'r') as zip_ref:  # Extracts the downloaded zip file\n",
        "    zip_ref.extractall(\".\")\n",
        "\n",
        "download_url = \"https://drive.google.com/uc?id=1wtmT_vH9mMUNOwrNOMFP6WFw6e8rbOdu\"\n",
        "output_zip = \"dlav_val.zip\"\n",
        "gdown.download(download_url, output_zip, quiet=False)\n",
        "with zipfile.ZipFile(output_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\".\")\n",
        "\n",
        "download_url = \"https://drive.google.com/uc?id=1G9xGE7s-Ikvvc2-LZTUyuzhWAlNdLTLV\"\n",
        "output_zip = \"dlav_test_public.zip\"\n",
        "gdown.download(download_url, output_zip, quiet=False)\n",
        "with zipfile.ZipFile(output_zip, 'r') as zip_ref:\n",
        "    zip_ref.extractall(\".\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TptnDCxT7ATM"
      },
      "source": [
        "## 📂 The Dataset\n",
        "\n",
        "We are now working with a richer dataset that includes not just images and trajectories,\n",
        "but also **depth maps** (and semantic segmentation labels, though unused in this example).\n",
        "\n",
        "The data is stored in `.pkl` files and each file contains:\n",
        "- `camera`: RGB image (shape: H x W x 3)\n",
        "- `sdc_history_feature`: the past trajectory of the car\n",
        "- `sdc_future_feature`: the future trajectory to predict\n",
        "- `depth`: ground truth depth map (shape: H x W x 1)\n",
        "\n",
        "We'll define a `DrivingDataset` class to load and return these tensors in a format our model can work with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KZGKlrP86QtM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import pickle\n",
        "\n",
        "class DrivingDataset(Dataset):\n",
        "    def __init__(self, file_list, test=False):\n",
        "        self.samples = file_list\n",
        "        self.test = test\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load pickle file\n",
        "        with open(self.samples[idx], 'rb') as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        # Convert numpy arrays to tensors\n",
        "        camera = torch.FloatTensor(data['camera']).permute(2, 0, 1)\n",
        "        history = torch.FloatTensor(data['sdc_history_feature'])\n",
        "        command = self.process_command(data['driving_command'])\n",
        "        semantic = self.process_semantic(data['semantic_label'])\n",
        "        depth = torch.FloatTensor(data['depth']).permute(2, 0, 1)\n",
        "\n",
        "        if not self.test:\n",
        "          future = torch.FloatTensor(data['sdc_future_feature'])\n",
        "          return {\n",
        "            'camera': camera,\n",
        "            'history': history,\n",
        "            'future': future,\n",
        "            'command': command,\n",
        "            'semantic': semantic,\n",
        "            'depth': depth\n",
        "          }\n",
        "        else:\n",
        "          return {\n",
        "            'camera': camera,\n",
        "            'history': history,\n",
        "            'semantic': semantic,\n",
        "            'command': command,\n",
        "            'depth': depth\n",
        "          }\n",
        "\n",
        "    def process_semantic(self, label):\n",
        "        num_classes = 63\n",
        "        semantic_map = torch.zeros((num_classes, label.shape[0], label.shape[1]), dtype=torch.float32)\n",
        "        for class_id in range(num_classes):\n",
        "            semantic_map[class_id] = (torch.from_numpy(label) == class_id).float()\n",
        "        lane_line_class = 12\n",
        "        semantic_map[lane_line_class] *= 2.0  #\n",
        "        return semantic_map\n",
        "\n",
        "    def process_command(self, cmd_str):\n",
        "        #\n",
        "        cmd_map = {'forward': [1, 0, 0], 'left': [0, 1, 0], 'right': [0, 0, 1]}\n",
        "        return torch.tensor(cmd_map[cmd_str], dtype=torch.float32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize the dataset"
      ],
      "metadata": {
        "id": "PU_kTAI8o5-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "k = 4\n",
        "# load the data\n",
        "data = []\n",
        "for i in random.choices(np.arange(1000), k=k):\n",
        "    with open(f\"train/{i}.pkl\", \"rb\") as f:\n",
        "        data.append(pickle.load(f))\n",
        "\n",
        "# plot the camera view of current step for the k examples\n",
        "fig, axis = plt.subplots(1, k, figsize=(4*k, 4))\n",
        "for i in range(k):\n",
        "    axis[i].imshow(data[i][\"camera\"])\n",
        "    axis[i].axis(\"off\")\n",
        "plt.suptitle(\"Camera view\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# plot the past and future trajectory of the vehicle\n",
        "fig, axis = plt.subplots(1, k, figsize=(4*k, 4))\n",
        "for i in range(k):\n",
        "    axis[i].plot(data[i][\"sdc_history_feature\"][:, 0], data[i][\"sdc_history_feature\"][:, 1], \"o-\", color=\"gold\", label=\"Past\")\n",
        "    axis[i].plot(data[i][\"sdc_future_feature\"][:, 0], data[i][\"sdc_future_feature\"][:, 1], \"o-\", color=\"green\", label=\"Future\")\n",
        "    axis[i].legend()\n",
        "    axis[i].axis(\"equal\")\n",
        "plt.suptitle(\"Past & future trajectories\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# plot feature space\n",
        "semantic_colormap = {\n",
        "    0: (0, 0, 0),         # UNLABELED\n",
        "    1: (0, 0, 142),       # CAR\n",
        "    2: (0, 0, 70),        # TRUCK\n",
        "    3: (220, 20, 60),     # PEDESTRIAN\n",
        "    4: (119, 11, 32),     # BIKE\n",
        "    5: (152, 251, 152),   # TERRAIN\n",
        "    6: (128, 64, 128),    # ROAD\n",
        "    7: (244, 35, 232),    # SIDEWALK\n",
        "    8: (70, 130, 180),    # SKY\n",
        "    9: (250, 170, 30),    # TRAFFIC_LIGHT\n",
        "    10: (190, 153, 153),  # FENCE\n",
        "    11: (220, 220, 0),    # TRAFFIC_SIGN\n",
        "    12: (255, 255, 255),  # LANE_LINE\n",
        "    13: (55, 176, 189),   # CROSSWALK\n",
        "    14: (0, 60, 100)      # BUS\n",
        "}\n",
        "\n",
        "fig, axis = plt.subplots(1, k, figsize=(4*k, 4))\n",
        "for i in range(k):\n",
        "    axis[i].imshow(data[i][\"semantic_label\"])\n",
        "    axis[i].axis(\"off\")\n",
        "plt.suptitle(\"Semantic features\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "fig, axis = plt.subplots(1, k, figsize=(4*k, 4))\n",
        "ims = []\n",
        "for i in range(k):\n",
        "    im = axis[i].imshow(data[i][\"depth\"])\n",
        "    axis[i].axis(\"off\")\n",
        "    ims.append(im)\n",
        "\n",
        "plt.suptitle(\"Depth\")\n",
        "plt.tight_layout()\n",
        "fig.colorbar(ims[-1], shrink=0.6)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "5q9ndN9Wo4_K",
        "outputId": "32300aa6-4889-4554-b355-b3b3bd4355db"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'train/974.pkl'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4ece62f3528a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"train/{i}.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train/974.pkl'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Logger:\n",
        "    def __init__(self):\n",
        "        # Placeholder for potential future configs (e.g., log_dir, wandb_enabled, etc.)\n",
        "        self.train_loss = []\n",
        "        self.val_loss = []\n",
        "        self.ADE = []\n",
        "        self.FDE = []\n",
        "        self.MSE = []\n",
        "\n",
        "    def log(self, step=None, **metrics):\n",
        "        \"\"\"\n",
        "        Logs the given metrics.\n",
        "\n",
        "        Args:\n",
        "            step (int, optional): The current step or epoch. Useful for tracking.\n",
        "            **metrics: Arbitrary keyword arguments representing metric names and values.\n",
        "        \"\"\"\n",
        "        prefix = f\"[Step {step}] \" if step is not None else \"\"\n",
        "        metric_str = \" | \".join(f\"{k}: {v}\" for k, v in metrics.items())\n",
        "        for k, v in metrics.items():\n",
        "            if k == \"train_loss\":\n",
        "                self.train_loss.append(v)\n",
        "            elif k == \"val_loss\":\n",
        "                self.val_loss.append(v)\n",
        "            elif k == \"ADE\":\n",
        "                self.ADE.append(v)\n",
        "            elif k == \"FDE\":\n",
        "                self.FDE.append(v)\n",
        "            elif k == \"MSE\":\n",
        "                self.MSE.append(v)\n",
        "        # print(prefix + metric_str)"
      ],
      "metadata": {
        "id": "Sa0HhAqP0NCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ-4uC6e7xdz"
      },
      "source": [
        "## 🧠 The Model: Trajectory + Depth Prediction\n",
        "\n",
        "We've extended our trajectory prediction model to optionally include a **depth estimation decoder**.\n",
        "\n",
        "Why?\n",
        "- Predicting depth helps the model **learn richer visual features** from the camera input.\n",
        "- This acts as a form of **multi-task learning**, where learning to estimate depth reinforces scene understanding, ultimately leading to better trajectory predictions.\n",
        "- This can be especially useful in complex environments with occlusions or sharp turns.\n",
        "\n",
        "The model has:\n",
        "- A CNN backbone to extract features from the image\n",
        "- An MLP to process historical trajectory features\n",
        "- A trajectory decoder to predict future coordinates\n",
        "- (Optionally) A depth decoder to predict dense depth maps\n",
        "\n",
        "This auxiliary task is enabled by setting `use_depth_aux=True`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5wLdpQzP7IY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DrivingPlanner(nn.Module):\n",
        "    def __init__(self, use_depth_aux=False):\n",
        "        super().__init__()\n",
        "        self.use_depth_aux = use_depth_aux\n",
        "\n",
        "        self.cnn_backbone = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=5, stride=2, padding=2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.cnn_backbone = nn.Sequential(\n",
        "            nn.Conv2d(3 , 32, 5, stride=2, padding=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 5, stride=2, padding=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 128, 5, stride=2, padding=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((8, 8)),\n",
        "            nn.Dropout(0.3)  # Preventing Overfitting\n",
        "        )\n",
        "\n",
        "        self.cnn_flatten = nn.Flatten()\n",
        "\n",
        "        self.history_encoder = nn.Sequential(\n",
        "            nn.Linear(21 * 3, 128),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.command_embedding = nn.Embedding(3, 32)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(8352, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, 60 * 3)  # Predict the trajectory of the next 60 steps\n",
        "        )\n",
        "\n",
        "        # Optional depth decoder\n",
        "        if self.use_depth_aux:\n",
        "            self.depth_decoder = nn.Sequential(\n",
        "                nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  # Upsample\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(64, 1, kernel_size=3, padding=1),\n",
        "                nn.Upsample(size=(200, 300), mode='bilinear', align_corners=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, camera, history, command):\n",
        "\n",
        "        img_feat = self.cnn_backbone(self.normalize_camera(camera))           # [32, 128, 8, 8]\n",
        "        # print(img_feat.shape)\n",
        "        img_feat_flat = self.cnn_flatten(img_feat)\n",
        "        hist_feat = self.history_encoder(history.view(history.size(0), -1))\n",
        "        command_label = torch.argmax(command, dim=1)\n",
        "        cmd_feat = self.command_embedding(command_label)\n",
        "\n",
        "        feat = torch.cat([img_feat_flat, hist_feat, cmd_feat], dim=-1)\n",
        "        future = self.fc(feat).view(-1, 60, 3)\n",
        "\n",
        "        # Optional depth map prediction\n",
        "        depth_out = None\n",
        "        if self.use_depth_aux:\n",
        "            depth_out = self.depth_decoder(img_feat.detach())\n",
        "            # print(depth_out.shape)\n",
        "\n",
        "\n",
        "        return future, depth_out\n",
        "\n",
        "    def normalize_camera(self, img):\n",
        "        # Normalized by ImageNet mean standard deviation\n",
        "        mean = torch.tensor([0.485, 0.456, 0.406], device=img.device).view(1, 3, 1, 1)\n",
        "        std = torch.tensor([0.229, 0.224, 0.225], device=img.device).view(1, 3, 1, 1)\n",
        "        return (img - mean) / std\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuOIwTuS7__h"
      },
      "source": [
        "## 🏋️ Training with Auxiliary Loss\n",
        "\n",
        "The training loop is similar to Phase 1 — except now, if enabled, we also compute a loss on the predicted **depth map**.\n",
        "\n",
        "We define:\n",
        "- `trajectory_loss` as standard MSE between predicted and true future trajectory\n",
        "- `depth_loss` as L1 loss between predicted and ground truth depth\n",
        "\n",
        "Total loss = `trajectory_loss + lambda * depth_loss`\n",
        "\n",
        "This helps guide the model to learn better representations from visual input. The weight `lambda` is a hyperparameter you can tune!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdmD0Tbd79mp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_one_epoch(model, train_loader, optimizer, device, lambda_depth=0.1, use_depth_aux=False):\n",
        "    model.train()\n",
        "    criterion = nn.SmoothL1Loss()  #\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        cam, hist, fut, depth, command = [batch[k].to(device) for k in ['camera', 'history', 'future', 'depth', 'command']]\n",
        "        optimizer.zero_grad()\n",
        "        fut_pred, dep_pred = model(cam, hist, command)\n",
        "\n",
        "        traj_loss = F.mse_loss(fut_pred, fut)\n",
        "        loss_heading = criterion(fut_pred[..., 2], fut[..., 2])\n",
        "        loss = traj_loss + 0.2 * loss_heading\n",
        "\n",
        "        if use_depth_aux:\n",
        "            loss += lambda_depth * F.l1_loss(dep_pred, depth)\n",
        "\n",
        "        is_turning = (command[:,1] == 1) | (command[:,2] == 1)  # left or right\n",
        "        weights = torch.where(is_turning, 1.5, 1.0).to(device)\n",
        "        loss = (loss * weights).mean()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    avg_loss = train_loss / len(train_loader)\n",
        "    return avg_loss\n",
        "\n",
        "def validate(model, val_loader, device, lambda_depth=0.1, use_depth_aux=False):\n",
        "    model.eval()\n",
        "    total_ade, total_fde, total_mse, total_loss = 0.0, 0.0, 0.0, 0.0\n",
        "    count = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            cam = batch['camera'].to(device)           # (batch, 3, H, W)\n",
        "            semantic = batch['semantic'].to(device)    # (batch, 15, H, W)\n",
        "            hist = batch['history'].to(device)         # (batch, 21, 3)\n",
        "            command = batch['command'].to(device)      # (batch, 3)\n",
        "            fut = batch['future'].to(device)           # (batch, 60, 3)\n",
        "            depth = batch['depth'].to(device)\n",
        "\n",
        "            fut_pred, dep_pred = model(cam, hist, command)\n",
        "\n",
        "            B, T, _ = fut.shape\n",
        "            count += B\n",
        "\n",
        "            ade = torch.norm(fut_pred[:, :, :2] - fut[:, :, :2], dim=2).mean(dim=1).sum()\n",
        "            fde = torch.norm(fut_pred[:, -1, :2] - fut[:, -1, :2], dim=1).sum()\n",
        "            mse = F.mse_loss(fut_pred, fut, reduction='sum')\n",
        "\n",
        "            if use_depth_aux:\n",
        "                loss = mse + lambda_depth * F.l1_loss(dep_pred, depth)\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            total_ade += ade.item()\n",
        "            total_fde += fde.item()\n",
        "            total_mse += mse.item()\n",
        "\n",
        "\n",
        "    ade_avg = total_ade / count\n",
        "    fde_avg = total_fde / count\n",
        "    mse_avg = total_mse / (count * T * 3)\n",
        "\n",
        "\n",
        "    loss_avg = total_loss / len(val_loader)\n",
        "    return ade_avg, fde_avg, mse_avg, loss_avg\n",
        "\n",
        "def train(model, train_loader, val_loader, optimizer, logger, num_epochs=50, lambda_depth=0.1, use_depth_aux=False):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_one_epoch(model, train_loader, optimizer, device, lambda_depth, use_depth_aux)\n",
        "        ade, fde, mse, val_loss = validate(model, val_loader, device)\n",
        "\n",
        "        if use_depth_aux:\n",
        "            logger.log(step=epoch, train_loss=train_loss, val_loss=val_loss, ADE=ade, FDE=fde, MSE=mse)\n",
        "            print(f\"Epoch {epoch+1}, Loss: {train_loss:.3f}, Validation - ADE: {ade:.3f}, FDE: {fde:.3f}, Traj MSE: {mse:.3f}, Loss (MSE+depth): {val_loss:.3f}\")\n",
        "        else:\n",
        "            logger.log(step=epoch, train_loss=train_loss, ADE=ade, FDE=fde, MSE=mse)\n",
        "            print(f\"Epoch {epoch+1}, Loss: {train_loss:.3f}, Validation - ADE: {ade:.3f}, FDE: {fde:.3f}, Traj MSE: {mse:.3f}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7HBGrjA8F4D"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "\n",
        "train_data_dir = \"train\"\n",
        "val_data_dir = \"val\"\n",
        "\n",
        "train_files = [os.path.join(train_data_dir, f) for f in os.listdir(train_data_dir) if f.endswith('.pkl')]\n",
        "val_files = [os.path.join(val_data_dir, f) for f in os.listdir(val_data_dir) if f.endswith('.pkl')]\n",
        "\n",
        "train_dataset = DrivingDataset(train_files)\n",
        "val_dataset = DrivingDataset(val_files)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, num_workers=2, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, num_workers=2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9De5eufP7IZ"
      },
      "source": [
        "## 🔍 Let's Compare Two Settings\n",
        "\n",
        "We'll now train and evaluate the model in **two modes**:\n",
        "\n",
        "1. **Without auxiliary task** — the model only predicts the trajectory.\n",
        "2. **With depth auxiliary task** — the model also predicts a depth map, which helps it learn better visual features.\n",
        "\n",
        "By comparing the results (ADE, FDE, and Trajectory MSE), you'll see the benefit of multi-task learning in action! 🚀"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJKcpkbV9bgX",
        "outputId": "2dbdad09-e2bd-4e4e-d7b6-7271bb66cc08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch 1, Loss: 22.9186, Validation - ADE: 4.0099, FDE: 9.3770, Traj MSE: 12.251686\n",
            "Epoch 2, Loss: 12.1647, Validation - ADE: 3.3641, FDE: 8.3131, Traj MSE: 9.574406\n",
            "Epoch 3, Loss: 9.8522, Validation - ADE: 3.4812, FDE: 8.5076, Traj MSE: 9.062673\n",
            "Epoch 4, Loss: 7.8995, Validation - ADE: 2.9848, FDE: 7.4598, Traj MSE: 7.337304\n",
            "Epoch 5, Loss: 6.8192, Validation - ADE: 2.4812, FDE: 6.5501, Traj MSE: 5.998971\n",
            "Epoch 6, Loss: 6.2871, Validation - ADE: 2.4374, FDE: 6.6189, Traj MSE: 5.879153\n",
            "Epoch 7, Loss: 6.1018, Validation - ADE: 2.7324, FDE: 7.0414, Traj MSE: 6.175226\n",
            "Epoch 8, Loss: 5.8419, Validation - ADE: 2.9170, FDE: 7.3691, Traj MSE: 6.510932\n",
            "Epoch 9, Loss: 5.6277, Validation - ADE: 2.4065, FDE: 6.4122, Traj MSE: 5.614890\n",
            "Epoch 10, Loss: 5.3516, Validation - ADE: 2.4830, FDE: 6.5718, Traj MSE: 5.395532\n"
          ]
        }
      ],
      "source": [
        "use_depth_aux=False\n",
        "model_no_aux = DrivingPlanner(use_depth_aux=use_depth_aux)\n",
        "optimizer = optim.Adam(model_no_aux.parameters(), lr=1e-3)\n",
        "logger_no_aux = Logger()\n",
        "train(model_no_aux, train_loader, val_loader, optimizer, logger_no_aux, num_epochs=10,use_depth_aux=use_depth_aux)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "id": "8vwFlM2D-wqo",
        "outputId": "1c73f698-098f-429f-aeeb-c6f6386c03ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 4, got 3)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-318c7c5003a8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_with_aux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlogger_aux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_with_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muse_depth_aux\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_depth_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-6-fce5bfde1882>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, optimizer, logger, num_epochs, lambda_depth, use_depth_aux)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_depth_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0made\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfde\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mADE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0made\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFDE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfde\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}, Loss: {train_loss:.3f}, Validation - ADE: {ade:.3f}, FDE: {fde:.3f}, Traj MSE: {mse:.3f}, Loss (MSE+depth): {val_loss:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
          ]
        }
      ],
      "source": [
        "use_depth_aux=True\n",
        "model_with_aux = DrivingPlanner(use_depth_aux=use_depth_aux)\n",
        "optimizer = optim.Adam(model_with_aux.parameters(), lr=1e-3)\n",
        "logger_aux = Logger()\n",
        "train(model_with_aux, train_loader, val_loader, optimizer, logger_aux, num_epochs=50,use_depth_aux=use_depth_aux, lambda_depth=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot 4 losses in 4 plots\n",
        "fig, axis = plt.subplots(1, 4, figsize=(16, 4))\n",
        "axis[0].plot(logger_no_aux.train_loss)\n",
        "axis[0].set_title(\"Train Loss\")\n",
        "axis[1].plot(logger_no_aux.MSE)\n",
        "axis[1].set_title(\"MSE\")\n",
        "axis[2].plot(logger_no_aux.ADE)\n",
        "axis[2].set_title(\"ADE\")\n",
        "axis[3].plot(logger_no_aux.FDE)\n",
        "axis[3].set_title(\"FDE\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O5wKWgkmGlEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_with_aux.state_dict(), \"phase2_model_w_depth.pth\")\n",
        "from google.colab import files\n",
        "files.download(\"phase2_model_w_depth.pth\")"
      ],
      "metadata": {
        "id": "LMVivXeK9k2y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wq-1VXMUP7IZ",
        "outputId": "a7e22fb0-8410-4ed1-ea32-cce272ac3677",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 438
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation results for model without depth auxiliary loss: ADE: 2.4830, FDE: 6.5718, Traj MSE: 5.395532\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected 3D (unbatched) or 4D (batched) input to conv_transpose2d, but got input of size: [32, 8352]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-a8fafba6c428>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation results for model without depth auxiliary loss: ADE: {ade:.4f}, FDE: {fde:.4f}, Traj MSE: {mse:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0made\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfde\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_with_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Validation results for model with depth auxiliary loss: ADE: {ade:.4f}, FDE: {fde:.4f}, Traj MSE: {mse:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-ceb9b882b376>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, val_loader, device, lambda_depth, use_depth_aux)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'depth'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m             \u001b[0mfut_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdep_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-d62642e439ea>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, camera, history, command)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mdepth_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_depth_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mdepth_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         )\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m         return F.conv_transpose2d(\n\u001b[0m\u001b[1;32m   1163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv_transpose2d, but got input of size: [32, 8352]"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ade, fde, mse = validate(model_no_aux, val_loader, device)\n",
        "print(f\"Validation results for model without depth auxiliary loss: ADE: {ade:.4f}, FDE: {fde:.4f}, Traj MSE: {mse:.6f}\")\n",
        "\n",
        "ade, fde, mse, val_loss = validate(model_with_aux, val_loader, device)\n",
        "print(f\"Validation results for model with depth auxiliary loss: ADE: {ade:.4f}, FDE: {fde:.4f}, Traj MSE: {mse:.6f}, Val. loss (depth): {mse:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qghg11NiP7Ia"
      },
      "source": [
        "## 🔍 Final Visualization and Comparison\n",
        "\n",
        "Now that we’ve trained two models — one **with** the depth auxiliary task and one **without** — let’s visualize and compare their predictions.\n",
        "\n",
        "We’ll show:\n",
        "1. The **camera image** from selected validation examples\n",
        "2. The **past trajectory**, **ground-truth future**, and **predicted future** trajectory\n",
        "3. The **predicted vs. ground-truth depth maps** (only for the model trained with the auxiliary task)\n",
        "\n",
        "These visualizations help us understand:\n",
        "- Does the predicted trajectory better match the future when the depth task is included?\n",
        "- Is the predicted depth map reasonably accurate?\n",
        "\n",
        "Let’s see the difference! 📈"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URt6A3apP7Ia",
        "outputId": "f648ace4-5288-4423-ccbb-a210bf9a2123",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<built-in method size of Tensor object at 0x7e887ff0e270>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given transposed=1, weight of size [32, 16, 4, 4], expected input[32, 128, 8, 8] to have 32 channels, but got 128 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-166ecfa63a0d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;31m# 🔚 Call at the end after training both models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m \u001b[0mvisualize_comparison\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_no_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_with_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-46-166ecfa63a0d>\u001b[0m in \u001b[0;36mvisualize_comparison\u001b[0;34m(val_loader, model_no_aux, model_with_aux, device)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mpred_no_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_no_aux\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcamera\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mpred_with_aux\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_depth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_with_aux\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcamera\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mcamera\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcamera\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-55e81ec4e839>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, camera, history, command)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mdepth_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_depth_aux\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mdepth_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_feat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m   1160\u001b[0m         )\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1162\u001b[0;31m         return F.conv_transpose2d(\n\u001b[0m\u001b[1;32m   1163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given transposed=1, weight of size [32, 16, 4, 4], expected input[32, 128, 8, 8] to have 32 channels, but got 128 channels instead"
          ]
        }
      ],
      "source": [
        "from codeop import CommandCompiler\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "random.seed(40)\n",
        "\n",
        "def visualize_comparison(val_loader, model_no_aux, model_with_aux, device):\n",
        "    model_no_aux.eval()\n",
        "    model_with_aux.eval()\n",
        "    val_batch = next(iter(val_loader))\n",
        "\n",
        "    camera = val_batch['camera'].to(device)\n",
        "    history = val_batch['history'].to(device)\n",
        "    future = val_batch['future'].to(device)\n",
        "    depth = val_batch['depth'].to(device)\n",
        "    command = val_batch['command'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        pred_no_aux, _ = model_no_aux(camera, history, command)\n",
        "        pred_with_aux, pred_depth = model_with_aux(camera, history, command)\n",
        "\n",
        "    camera = camera.cpu().numpy()\n",
        "    history = history.cpu().numpy()\n",
        "    future = future.cpu().numpy()\n",
        "    pred_no_aux = pred_no_aux.cpu().numpy()\n",
        "    pred_with_aux = pred_with_aux.cpu().numpy()\n",
        "    depth = depth.cpu().numpy()\n",
        "    pred_depth = pred_depth.cpu().numpy() if pred_depth is not None else None\n",
        "\n",
        "    k = 4\n",
        "    indices = random.choices(np.arange(len(camera)), k=k)\n",
        "\n",
        "    # Show the input camera images\n",
        "    fig, ax = plt.subplots(1, k, figsize=(4 * k, 4))\n",
        "    for i, idx in enumerate(indices):\n",
        "        ax[i].imshow(camera[idx].transpose(1, 2, 0))\n",
        "        ax[i].set_title(f\"Example {i+1}\")\n",
        "        ax[i].axis(\"off\")\n",
        "    plt.suptitle(\"Camera Inputs\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Compare predicted trajectories\n",
        "    fig, ax = plt.subplots(2, k, figsize=(4 * k, 8))\n",
        "    for i, idx in enumerate(indices):\n",
        "        # Without aux\n",
        "        ax[0, i].plot(history[idx, :, 0], history[idx, :, 1], 'o-', label='Past', color='gold', markersize=4, linewidth=1.2)\n",
        "        ax[0, i].plot(future[idx, :, 0], future[idx, :, 1], 'o-', label='GT Future', color='green', markersize=4, linewidth=1.2)\n",
        "        ax[0, i].plot(pred_no_aux[idx, :, 0], pred_no_aux[idx, :, 1], 'o-', label='Pred (No Aux)', color='red', markersize=4, linewidth=1.2)\n",
        "        ax[0, i].set_title(\"No Depth Aux\")\n",
        "        ax[0, i].axis(\"equal\")\n",
        "\n",
        "        # With aux\n",
        "        ax[1, i].plot(history[idx, :, 0], history[idx, :, 1], 'o-', label='Past', color='gold', markersize=4, linewidth=1.2)\n",
        "        ax[1, i].plot(future[idx, :, 0], future[idx, :, 1], 'o-', label='GT Future', color='green', markersize=4, linewidth=1.2)\n",
        "        ax[1, i].plot(pred_with_aux[idx, :, 0], pred_with_aux[idx, :, 1], 'o-', label='Pred (With Aux)', color='blue', markersize=4, linewidth=1.2)\n",
        "        ax[1, i].set_title(\"With Depth Aux\")\n",
        "        ax[1, i].axis(\"equal\")\n",
        "\n",
        "    # Show full legend in a new figure\n",
        "    fig_legend = plt.figure(figsize=(8, 1))\n",
        "    legend_handles = [\n",
        "        plt.Line2D([0], [0], color='gold', marker='o', linestyle='-', markersize=5, label='Past'),\n",
        "        plt.Line2D([0], [0], color='green', marker='o', linestyle='-', markersize=5, label='GT Future'),\n",
        "        plt.Line2D([0], [0], color='red', marker='o', linestyle='-', markersize=5, label='Pred (No Aux)'),\n",
        "        plt.Line2D([0], [0], color='blue', marker='o', linestyle='-', markersize=5, label='Pred (With Aux)')\n",
        "    ]\n",
        "    fig_legend.legend(handles=legend_handles, loc='center', ncol=4)\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.suptitle(\"Trajectory Prediction: Without vs With Depth Aux Task\")\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "    # Show predicted vs GT depth (only for bottom row)\n",
        "    if pred_depth is not None:\n",
        "        fig, ax = plt.subplots(2, k, figsize=(4 * k, 6))\n",
        "        for i, idx in enumerate(indices):\n",
        "            ax[0, i].imshow(depth[idx, :, :, 0], cmap='viridis')\n",
        "            ax[0, i].set_title(\"GT Depth\", pad=10)\n",
        "            ax[0, i].axis(\"off\")\n",
        "            # increase vertical distance between rows\n",
        "\n",
        "            ax[1, i].imshow(pred_depth[idx, :, :, 0], cmap='viridis')\n",
        "            ax[1, i].set_title(\"Pred Depth\", pad=10)\n",
        "            ax[1, i].axis(\"off\")\n",
        "\n",
        "        plt.suptitle(\"Depth Estimation (Only for Model With Aux Task)\", y=1.05)\n",
        "        plt.subplots_adjust(hspace=0.4)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# 🔚 Call at the end after training both models\n",
        "visualize_comparison(val_loader, model_no_aux, model_with_aux, device=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3P78BQXdP7Ia"
      },
      "source": [
        "Now we run our model on the test set once, to get the plan of our model and save it for submission. Notice that the ground truth plans are removed for the test set, so you can not calculate the ADE metric on the test set yourself, and need to submit it to the leader board. By running the last cell, you'll be able to see a csv file called submission_phase2.csv by clicking on the folder icon on the left. Download it and submit it to the leaderboard to get your score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDjhfBrCP7Ia",
        "outputId": "f8e1886e-a3aa-4575-d5f7-c37ca934633f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['camera', 'depth', 'driving_command', 'sdc_history_feature', 'semantic_label'])\n"
          ]
        }
      ],
      "source": [
        "with open(f\"test_public/0.pkl\", \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "print(data.keys())\n",
        "# Note the absence of sdc_future_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-JqhIfNP7Ib",
        "outputId": "c526f181-79e8-49e3-fcda-d9453e683840",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of df_xy: (1000, 121)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "test_data_dir = \"test_public\"\n",
        "test_files = [os.path.join(test_data_dir, fn) for fn in sorted([f for f in os.listdir(test_data_dir) if f.endswith(\".pkl\")], key=lambda fn: int(os.path.splitext(fn)[0]))]\n",
        "test_dataset = DrivingDataset(test_files, test=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=250, num_workers=2)\n",
        "model_with_aux.eval()\n",
        "all_plans = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        camera = batch['camera'].to(device)\n",
        "        history = batch['history'].to(device)\n",
        "\n",
        "        pred_future, _ = model_with_aux(camera, history)\n",
        "        all_plans.append(pred_future.cpu().numpy()[..., :2])\n",
        "all_plans = np.concatenate(all_plans, axis=0)\n",
        "\n",
        "# Now save the plans as a csv file\n",
        "pred_xy = all_plans[..., :2]  # shape: (total_samples, T, 2)\n",
        "\n",
        "# Flatten to (total_samples, T*2)\n",
        "total_samples, T, D = pred_xy.shape\n",
        "pred_xy_flat = pred_xy.reshape(total_samples, T * D)\n",
        "\n",
        "# Build a DataFrame with an ID column\n",
        "ids = np.arange(total_samples)\n",
        "df_xy = pd.DataFrame(pred_xy_flat)\n",
        "df_xy.insert(0, \"id\", ids)\n",
        "\n",
        "# Column names: id, x_1, y_1, x_2, y_2, ..., x_T, y_T\n",
        "new_col_names = [\"id\"]\n",
        "for t in range(1, T + 1):\n",
        "    new_col_names.append(f\"x_{t}\")\n",
        "    new_col_names.append(f\"y_{t}\")\n",
        "df_xy.columns = new_col_names\n",
        "\n",
        "# Save to CSV\n",
        "df_xy.to_csv(\"submission_phase2.csv\", index=False)\n",
        "\n",
        "print(f\"Shape of df_xy: {df_xy.shape}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}